{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE_Transferaufgabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.embeddings import BertEmbeddings, ELMoEmbeddings, FastTextEmbeddings, StackedEmbeddings, TransformerWordEmbeddings, WordEmbeddings\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "punctuation = ['!', '#','$','%','&', \"'\", '(',')','*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', \n",
    "               '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '`', '``', 'wurde', 'wurden']\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SPLIT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>verified</th>\n",
       "      <th>vote</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Mike L</td>\n",
       "      <td>Bought for Christmas present for my Grandson h...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01.02.2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Gustavo Villalta Woltke</td>\n",
       "      <td>Broken in  months</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.05.2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>David</td>\n",
       "      <td>The latest driver for this product on the Asus...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.05.2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                     name  \\\n",
       "0     1.0                   Mike L   \n",
       "1     1.0  Gustavo Villalta Woltke   \n",
       "2     1.0                    David   \n",
       "\n",
       "                                              review  verified  vote  \\\n",
       "0  Bought for Christmas present for my Grandson h...      True   0.0   \n",
       "1                                  Broken in  months      True   0.0   \n",
       "2  The latest driver for this product on the Asus...     False   0.0   \n",
       "\n",
       "         date  \n",
       "0  01.02.2018  \n",
       "1  23.05.2018  \n",
       "2  15.05.2018  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv(\"../corpora/small_amazon_reviews_electronic.csv\")\n",
    "corpus.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_jsonl(df, filename, text_col=\"review\", output_path=\"../corpora/splits/\"):\n",
    "    \"\"\" DataFrame with text column to Json Line Format. \"\"\"\n",
    "\n",
    "    df[text_col] = df.apply(lambda row: word_tokenize(row[text_col]), axis=1)\n",
    "    df.to_json(f\"{output_path}{filename}.json\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_corpus(corpus, \n",
    "                 text_col = \"review\", \n",
    "                 label_col = \"rating\", \n",
    "                 split = 0.8,\n",
    "                 output_path = \"../corpora/splits/\"):\n",
    "    \"\"\" Splits corpus in Train, Val and Test set and saves them \n",
    "        as jsonl files.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, X_remain = train_test_split(corpus, \n",
    "                                         train_size=split,\n",
    "                                         stratify=corpus[label_col])\n",
    "\n",
    "    val_test_split = int((corpus.shape[0] * 0.2)/2)\n",
    "    X_val = X_remain[:val_test_split]\n",
    "    X_test = X_remain[val_test_split:]\n",
    "\n",
    "\n",
    "\n",
    "    df_to_jsonl(X_train, \"train\", text_col = text_col, output_path = output_path)\n",
    "    df_to_jsonl(X_val, \"val\")\n",
    "    df_to_jsonl(X_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\" Returns accuracy per batch. \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "if SMALL_SPLIT:\n",
    "    corpus = corpus.sample(1000)\n",
    "    split_corpus(corpus, output_path = \"../corpora/splitssmall/\")\n",
    "else:\n",
    "    split_corpus(corpus, split=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparams and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 2\n",
    "\n",
    "EMBEDDING_TYPE = \"glove.6B.100d\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english') + punctuation \n",
    "\n",
    "REVIEW = data.Field(tokenize = \"toktok\",\n",
    "                    lower = True,\n",
    "                    stop_words=stop_words)\n",
    "\n",
    "RATING = data.LabelField()\n",
    "assigned_fields = {\"review\": ('text', REVIEW), \n",
    "                   \"rating\": ('label', RATING)}\n",
    "\n",
    "train_data, val_data, test_data = data.TabularDataset.splits(path=\"../corpora/splits/\", \n",
    "                                                              train='train.json',\n",
    "                                                              validation='val.json', \n",
    "                                                              test='test.json', \n",
    "                                                              format='json',\n",
    "                                                              fields=assigned_fields,\n",
    "                                                              skip_header = True)\n",
    "\n",
    "\n",
    "\n",
    "REVIEW.build_vocab(train_data, \n",
    "                   #vectors = EMBEDDING_TYPE, \n",
    "                   #unk_init = torch.Tensor.normal_,\n",
    "                   max_size = MAX_VOCAB_SIZE)\n",
    "RATING.build_vocab(train_data)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, val_iterator, test_iterator = data.BucketIterator.splits((train_data, val_data, test_data), \n",
    "                                                                         batch_size = BATCH_SIZE,\n",
    "                                                                         device = device,\n",
    "                                                                         sort_key = lambda x: len(x.text),\n",
    "                                                                         sort = False,\n",
    "                                                                         sort_within_batch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = Sentence(' '.join(REVIEW.vocab.itos))\n",
    "glove_embeddings = WordEmbeddings('glove').embed(sentence)\n",
    "#vectors = torch.tensor(2)\n",
    "#embedding = nn.Embedding.from_pretrained(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module): #erbt immer von nn.Module\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        text = text.permute(1, 0)\n",
    "        embedded = self.embedding(text)  \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(REVIEW.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2,3,4]\n",
    "OUTPUT_DIM = len(RATING.vocab)\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = REVIEW.vocab.stoi[RATING.pad_token]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "\n",
    "\n",
    "OPTIMIZER = optim.Adam(model.parameters())\n",
    "CRITERION = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = REVIEW.vocab.vectors #load embeddings\n",
    "#model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "UNK_IDX = REVIEW.vocab.stoi[REVIEW.unk_token]\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put model and loss criterion to device (cpu or gpu)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train() # Model lernt was (Zustand des Modells)\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad() # Gradienten müssen auf 0 gesetzt werden\n",
    "        predictions = model(batch.text)\n",
    "        loss = criterion(predictions, batch.label) # berechnet loss (y-y^)\n",
    "        acc = categorical_accuracy(predictions, batch.label)\n",
    "        loss.backward() # sammelt Gradienten für jeden Parameter\n",
    "        optimizer.step() # updated parameter basierend auf den Gradienten\n",
    "        \n",
    "        epoch_loss += loss.item() # item extrahiert den loss Wert\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval() # Gewichte sind hier eingefroren\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = categorical_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 23s\n",
      "\tTrain Loss: 1.876 | Train Acc: 20.88%\n",
      "\t Val. Loss: 1.767 |  Val. Acc: 20.75%\n",
      "Epoch: 02 | Epoch Time: 1m 54s\n",
      "\tTrain Loss: 1.868 | Train Acc: 20.46%\n",
      "\t Val. Loss: 1.767 |  Val. Acc: 20.75%\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, OPTIMIZER, CRITERION)\n",
    "    valid_loss, valid_acc = evaluate(model, val_iterator, CRITERION)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        # TODO: string anpassen\n",
    "        torch.save(model.state_dict(), 'savefiles/cnnmodel.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.880 | Test Acc: 17.24%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('savefiles/cnnmodel.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, CRITERION)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_class(model, sentence, min_len = 4):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [REVIEW.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    preds = model(tensor)\n",
    "    max_preds = preds.argmax(dim = 1)\n",
    "    return max_preds.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class is: 1\n"
     ]
    }
   ],
   "source": [
    "pred_class = predict_class(model, \"\")\n",
    "print(f'Predicted class is: {pred_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['works', 'as', 'advertisedbut', 'really', 'do', 'not', 'use', 'it', 'that', 'much', 'maybe', 'some', 'handy', 'the', 'future']\n",
      "\n",
      "Einzigarte Tokens im REVIEW-Vokabular: 25002\n",
      "Einzigarte Tokens im RATING-Vokabular: 5\n",
      "\n",
      "[('the', 155453), ('i', 94375), ('to', 84631), ('it', 76278), ('a', 75353), ('and', 75229), ('not', 57672), ('is', 50266), ('for', 39894), ('this', 39379), ('of', 39150), ('my', 31541), ('that', 29659), ('but', 28925), ('with', 28467), ('on', 27336), ('have', 26603), ('you', 23607), ('as', 18550), ('are', 16893)]\n",
      "\n",
      "['<unk>', '<pad>', 'the', 'i', 'to', 'it', 'a', 'and', 'not', 'is']\n",
      "\n",
      "tensor([[342,  11,  41,  ...,   8, 149,   3],\n",
      "        [ 56,   9,  28,  ..., 708,   1, 152],\n",
      "        [ 24,  13,   1,  ..., 148,   1,  86],\n",
      "        ...,\n",
      "        [  1,   1,   1,  ...,   1,   1,   1],\n",
      "        [  1,   1,   1,  ...,   1,   1,   1],\n",
      "        [  1,   1,   1,  ...,   1,   1,   1]])\n",
      "\n",
      "tensor([1, 1, 4, 2, 0, 2, 0, 3, 2, 4, 0, 3, 1, 3, 3, 1, 4, 1, 4, 4, 3, 3, 0, 1,\n",
      "        4, 3, 1, 1, 1, 0, 2, 1, 3, 1, 3, 0, 2, 0, 4, 0, 1, 0, 0, 3, 2, 3, 2, 2,\n",
      "        3, 3, 1, 1, 0, 1, 3, 1, 0, 0, 1, 3, 4, 1, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "example = vars(train_data.examples[0])\n",
    "print(example[\"review\"][:20])\n",
    "print()\n",
    "print(f\"Einzigarte Tokens im REVIEW-Vokabular: {len(REVIEW.vocab)}\")\n",
    "print(f\"Einzigarte Tokens im RATING-Vokabular: {len(RATING.vocab)}\")\n",
    "print()\n",
    "print(REVIEW.vocab.freqs.most_common(20))\n",
    "print()\n",
    "print(REVIEW.vocab.itos[:10])\n",
    "print()\n",
    "batch = next(iter(train_iterator))\n",
    "print(batch.review)\n",
    "print()\n",
    "print(batch.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(([[3,5], [5, 1], [10,2]]), dtype=float)\n",
    "y = np.array(([75], [82], [93]), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "\n",
    "X = X / np.amax(X, axis=0)\n",
    "y = y/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3, 1. ],\n",
       "       [0.5, 0.2],\n",
       "       [1. , 0.4]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75],\n",
       "       [0.82],\n",
       "       [0.93]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoidPrime(x):\n",
    "    return np.exp(-x)/((1+np.exp(-x))**2)\n",
    "\n",
    "\n",
    "class NN(object):\n",
    "    def __init__(self):\n",
    "        self.inputLayerSize = 2\n",
    "        self.hiddenLayerSize = 3\n",
    "        self.outputLayerSize = 1\n",
    "        \n",
    "        self.W1 = np.random.rand(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        self.W2 = np.random.rand(self.hiddenLayerSize, self.outputLayerSize)\n",
    "        \n",
    "        #self.W1 = np.zeros((self.inputLayerSize, self.hiddenLayerSize))\n",
    "        #self.W2 = np.zeros(((self.hiddenLayerSize, self.outputLayerSize)))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.z2 = np.dot(X, self.W1) # broadcasting\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        self.y_hat = sigmoid(self.z3)\n",
    "        return self.y_hat\n",
    "    \n",
    "    \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NN(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69816825],\n",
       "       [0.68668206],\n",
       "       [0.70690268]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((2,2), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bs128_mf25000_lr0.01', 71.28666666666668),\n",
       " ('bs50_mf25000_lr0.01', 71.20666666666666),\n",
       " ('bs50_mf50000_lr0.01', 71.09666666666668),\n",
       " ('bs128_mf50000_lr0.01', 70.86666666666666),\n",
       " ('bs50_mf25000_lr0.001', 70.76666666666667),\n",
       " ('bs50_mf50000_lr0.001', 70.66000000000001),\n",
       " ('bs128_mf50000_lr0.001', 69.89),\n",
       " ('bs128_mf25000_lr0.001', 69.12333333333333),\n",
       " ('bs50_mf25000_lr0.1', 65.73333333333333),\n",
       " ('bs50_mf50000_lr0.1', 65.61)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_name = \"fasttext_simple\"\n",
    "path = f\"../results/fasttext/{dir_name}\"\n",
    "\n",
    "results = defaultdict(list)\n",
    "for file in Path(path).rglob(\"*.txt\"):\n",
    "    with open(file, \"r\") as f:\n",
    "        s = f.read()\n",
    "        s = s[-6:-1]\n",
    "        s = s.replace(\" \", \"\")\n",
    "        s = float(s)\n",
    "    key = file.stem[len(dir_name)+3:]\n",
    "    results[key].append(s)\n",
    "    \n",
    "means = {}\n",
    "for k, v in dict(results).items():\n",
    "    means[k] = np.mean(v)\n",
    "means_sorted = sorted(means.items(), key=lambda x: x[1], reverse=True)\n",
    "means_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
