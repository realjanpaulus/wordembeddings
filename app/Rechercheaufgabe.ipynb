{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOS:<br>\n",
    "\n",
    "- Übersicht über alle Verfahren geben in kapitel 3 (Tabelle?!)\n",
    "- überprüfen Formulierung\n",
    "- untertitel der grafik bei LSA in pdf überprüfen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Inhaltsverzeichnis<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Einführung\" data-toc-modified-id=\"Einführung-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Einführung</a></span><ul class=\"toc-item\"><li><span><a href=\"#Das-Bag-of-Words-Modell\" data-toc-modified-id=\"Das-Bag-of-Words-Modell-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Das Bag-of-Words Modell</a></span></li><li><span><a href=\"#Grenzen-des-Bag-of-Words-Modells\" data-toc-modified-id=\"Grenzen-des-Bag-of-Words-Modells-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Grenzen des Bag-of-Words Modells</a></span></li></ul></li><li><span><a href=\"#LSA\" data-toc-modified-id=\"LSA-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>LSA</a></span></li><li><span><a href=\"#Word-Embeddings\" data-toc-modified-id=\"Word-Embeddings-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Word Embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Word2Vec</a></span><ul class=\"toc-item\"><li><span><a href=\"#Vortrainierte-Embeddings-verwenden\" data-toc-modified-id=\"Vortrainierte-Embeddings-verwenden-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Vortrainierte Embeddings verwenden</a></span></li><li><span><a href=\"#Eigene-Embeddings-verwenden\" data-toc-modified-id=\"Eigene-Embeddings-verwenden-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Eigene Embeddings verwenden</a></span></li></ul></li><li><span><a href=\"#Glove\" data-toc-modified-id=\"Glove-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Glove</a></span></li><li><span><a href=\"#FastText\" data-toc-modified-id=\"FastText-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>FastText</a></span></li><li><span><a href=\"#ELMo\" data-toc-modified-id=\"ELMo-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>ELMo</a></span></li><li><span><a href=\"#BERT\" data-toc-modified-id=\"BERT-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>BERT</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_word_embeddings' from 'utils' (/Users/jan/Desktop/informatik_programme/wordembeddings/app/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-4ef267022d8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_word_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_word_embeddings' from 'utils' (/Users/jan/Desktop/informatik_programme/wordembeddings/app/utils.py)"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as gensim_downloader\n",
    "\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import urllib.request\n",
    "\n",
    "from utils import preprocess_text, plot_word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_w2v = [\"das kind sagt, dass es feuerwehrmann sein möchte\",\n",
    "            \"der erwachsene sagt, dass er lieber kein feuerwehrmann sein möchte\",\n",
    "            \"das kind fragt, was der erwachsene denn lieber sein möchte\",\n",
    "            \"der erwachsene sagt, dass er lieber polizist sein möchte\"]\n",
    "#text_w2v = [word_tokenize(sen) for sen in text_w2v]\n",
    "df = pd.DataFrame(text_w2v, columns = [\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corpus = pd.read_csv(\"../corpora/small_amazon_reviews_electronic.csv\")\n",
    "corpus[\"review\"] = corpus.review.apply(lambda x: preprocess_text(x))\n",
    "texts = [word_tokenize(row[\"review\"]) for idx, row in corpus.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://cloud.devmount.de/d2bc5672c523b086/german.model'\n",
    "urllib.request.urlretrieve(url, \"german_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einführung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das **Natural Language Processing** (kurz: NLP) befasst sich mit Methoden und Verfahren zur maschinellen Verarbeitung von natürlicher Sprache in Form von Worten, Texten oder ganzen Korpora. Bevor jedoch NLP Verfahren wie die Textklassifikation oder das Topic Modelling auf die Textdaten angewendet werden können, müssen diese in eine Darstellungsweise umgewandelt werden, mit der die Verfahren arbeiten können. Die rohen Textdaten werden daher in **Vektoren**, die aus Zahlen bestehen, umgewandelt. Dieser Vorgang nennt sich **Vektorisierung**. Ein Wort wie \"Baum\" kann dadurch als Vektor aufgefasst werden. Natürlich können auch andere Features aus den Texten als Vektoren dargestellt werden; so ist es auch möglich, einzelne Buchstaben, Phrasen, Sätze, Segmente oder ganze Texte als Features aus den Textdaten zu extrahieren und diese zu vektorisieren. In der folgenden Übersicht werden jedoch vorwiegend Wörter als Features verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das Bag-of-Words Modell\n",
    "\n",
    "Das wohl einfachste Verfahren zur Darstellung von Wörtern als Vektoren ist das **Bag-of-Words** Modell. Wörter werden hier als eindimensionale Vektoren (= einfache Zahl) dargestellt, wobei jedes individuelle Wort einen individuellen eindimensionalen Vektor (auch: **Index**) zugeordnet bekommt. Die Zuordnungen jedes einzigartigen Wortes zu seinem Vektor werden in einem *Vokabular* gespeichert. Nun können mithilfe dieses Vokabulars auch ganze Sätze oder sogar Texte dargestellt werden. Dafür wird für jeden Satz/Text ein Vektor gebildet, der die gleiche Länge wie das Vokabular hat. Jedem Eintrag des Vektors wird anhand des Vokabulars ein Wort zugeordnet. Der Satz/Text wird dann als Vektor aus **absoluten Termhäufigkeiten** dargestellt, wo an jeder Stelle, an dem ein Wort aus dem Vokabular in dem Text vorkommt, die Häufigkeit des Wortes in dem jeweiligen Satz/Text steht und an jeder anderen Stelle eine $0$, da es kein einziges Mal vorkommt.[<sup>1</sup>](#fn1) Dies soll im Folgenden anhand eines Code-Beispiels erläutert werden. Zuerst wird das Vokabular aller Texte dargestellt, bei dem die Wörter einem Index zugeordnet werden (es wird ab $0$ gezählt). Danach werden die vektorisierten Sätze/Texte angezeigt.\n",
    "\n",
    "<hr style=\"border: 0.1px solid black;\"/>\n",
    "<div id=\"fn1\" style=\"font-size:8pt; line-height:1; padding-left: 1em; text-indent: -1em\"><sup style=\"font-size:5pt\">1</sup> &nbsp; Dies ist nur eine Möglichkeit, die Häufigkeit eines Wortes beim Bag-of-Words Modell darzustellen. Eine weitere Möglichkeit wären <b>binäre Häufigkeiten</b>, bei denen das Vorkommen eines Wortes mit einer $1$ und die Abwesenheit eines Wortes mit einer $0$ gekennzeichnet werden. Um häufigen Wörtern in den Dokumenten weniger Gewicht zu geben, da dies oft einen geringeren Informationsgehalt besitzen, ist es auch möglich, das Bag-of-Words Modell in der Kombination mit dem <b>TF-IDF Maß</b> aus dem Bereich des Information Retrievals zu verwenden, bei dem die Häufigkeit von Worten skaliert wird.</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ich': 10, 'gehe': 6, 'nachher': 16, 'zur': 24, 'bank': 1, 'um': 21, 'etwas': 5, 'geld': 7, 'zu': 23, 'holen': 8, 'möchte': 15, 'mich': 13, 'kurz': 11, 'auf': 0, 'die': 3, 'setzen': 19, 'mir': 14, 'essen': 4, 'stand': 20, 'von': 22, 'der': 2, 'hölzernen': 9, 'neben': 17, 'liegt': 12, 'noch': 18}\n"
     ]
    }
   ],
   "source": [
    "text = [\"ich gehe nachher zur bank, um etwas geld zu holen\",\n",
    "        \"ich möchte mich kurz auf die bank setzen\",\n",
    "        \"um mir etwas zu essen zu holen, stand ich von der bank auf\", \n",
    "        \"auf der hölzernen bank neben der bank liegt noch geld\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vector = vectorizer.fit_transform(text)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1]\n",
      " [1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0]\n",
      " [1 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 2 0]\n",
      " [1 2 2 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grenzen des Bag-of-Words Modells\n",
    "\n",
    "Aufgrund seiner Einfachheit ist das Bag-of-Words Modell gut verständlich und sehr schnell umsetzbar. Es hat jedoch eine Reihe an Nachteilen, von deinen einige im Folgenden kurz erläutert werden:\n",
    "\n",
    "- **Keine Informationen über Reihenfolge der Wörter.** Beim Bag-of-Words Modell wird jegliche Information über die Reihenfolge der Wörter verworfen, der Kontext eines Wortes bleibt unberücksichtigt. Dies wird auch durch den Namen dieses Modells deutlich: Die Bezeichnung \"bag\" (deutsch: Sack) soll darauf hinweisen, dass alle Informationen über die Struktur oder Reihenfolge der Wörter im Dokument verworfen werden, da sie metaphorisch in einen \"Sack\" geworfen werden. Die Reihenfolge lässt sich auch nicht im Nachhinein rekonstruieren. Insgesamt gehen somit sehr viele semantische Informationen verloren. Eine Lösung, bei der die Reihenfolge der Worte berücksichtigt werden kann, ist die Verwendung von **N-Grammen** oder **LSA** (siehe Kapitel 2 TODO). \n",
    "- **Spärlichkeit von Wortvektoren.** Umso mehr verschiedene Worte in den verwendeten Texten vorkommen, umso größer wird das Vokabular. Dies kann oft zu sehr spärlichen (engl. *sparse*) Wortvektoren führen. Besteht das Vokabular aus 500000 Worten, ein Text aber nur aus 50 verschiedenen Worten, sind nur 0.01% der Stellen des 500000 langen Wortvektors mit Einsen besetzt, der Rest nur mit Nullen. Dies führt dazu, dass eine große Menge an Rechenspeicher für die Verarbeitung der riesigen Matrizen benötigt wird. Weiterhin werden wenige Informationen in sehr großen Repräsentationsräumen benutzt, wodurch es für einige NLP Verfahren und Modelle problematisch ist, diese wenigen Information effizient zu nutzen. Eine Lösung bieten dichtbesetzte **Word Embeddings**, die in den nächsten Kapiteln behandelt werden.\n",
    "- **Abbildung der Mehrdeutigkeit von Worten**. Wörter können trotz gleicher Schreibweise mehrere Bedeutungen haben, welche sich durch den Kontext des Wortes zeigen können. Dies wird durch das Bag-of-Words Modell nicht abgebildet. Eine mögliche Lösung wäre die Verwendung von **kontextabhängigen Word Embeddings** wie die **BERT-Embeddings** in Kapitel 7 (TODO)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA\n",
    "\n",
    "**Latent Semantic Analysis** (kurz: LSA, auch: *Latent Semantic Indexing*) ist ein Verfahren aus dem Bereich des Information Retrievals aus dem Jahre 1990. Bei diesem Verfahren werden Dokumente und Terme (repräsentiert durch eine **Term-Dokument Matrix**) in einem latenten Raum abgebildet, der aus **Konzepten** (oder **Hauptkomponenten**) besteht. Dokumente, die ähnlich zueinander sind, d.h. aus ähnlichen Konzepten bestehen, werden in diesem Raum näher beieinander platziert. Dies wird durch die folgende Grafik deutlich.\n",
    "\n",
    "\n",
    "![lsa](img/lsa.png)\n",
    "\n",
    "Grafik von Susan Dumais, siehe <a href=\"http://www.ifis.uni-luebeck.de/~moeller/tuhh-lectures/mmieir-sose-12/05-Latent-Semantic-Analysis.pdf\">Präsentation</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ziel der LSA ist es, die Konzepte innerhalb der Dokumente zu finden. Dabei greift das Verfahren auf eine Technik der linearen Algebra zurück, der **Singulärwertzerlegung** (englisch: Singular Value Decomposition). Die Idee dabei ist, dass die Term-Dokument Matrix aus **Hauptdimensionen**, welche die wichtigen Konzepte der Dokumente beinhalten, und aus weniger aussagekräftigen Dimensionen mit unwichtigen Termen besteht. Mithilfe der Singulärwertzerlegung wird die originale Term-Dokument Matrix in drei Matrizen aufgeteilt, wobei die beiden äußeren Matrizen aus den linken bzw. rechten orthonormalen Eigenvektoren bestehen und die mittlere Matrix eine Diagonalmatrix ist, die die singulären Werte der Originalmatrix enthält. Mit Hilfe dieser Zerlegung kann eine **Approximation** der Originalmatrix mit einer kleiner dimensionierten Matrix erreicht werden. Die singulären Werte in der Diagonalmatrix sind nach ihrer Größe absteigend geordnet. Singulärwerte, die unter einem bestimmten Schwellenwert liegen, werden entfernt. Auch in den anderen Matrizen werden entsprechende Zeilen oder Spalten entfernt. Mit Hilfe der reduzierten Matrizen erhält man durch Matrixmultiplikation die optimale Approximation der Originalmatrix, die kleiner als die originale Term-Dokument Matrix ist, da Informationen aus den weniger aussagekräftigen Dimensionen verworfen wurden. Weiterhin werden bei der Dimensionsreduktion auch ähnliche Konzepte zusammengefasst, so werden z.B. Worte wie \"Tür\" und \"Tor\" in einem Konzept zusammengefasst.\n",
    "\n",
    "Der Vorteil der LSA ist, dass anders als beim Bag-of-Words Modell die **Semantik** der Dokumente widergegeben werden kann. Zudem werden **Synonyme** zusammengefasst. Probleme hat LSA jedoch mit der **Polysemie**. Zudem ist der Algorithmus sehr rechenaufwendig.\n",
    "\n",
    "TODO: mehr?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "**Word Embeddings** (deutsch: Worteinbettungen) sind die Sammelbezeichnung für eine Reihe von Sprachmodellierungstechniken. Anders als beim Bag-of-Words Modell werden Wörter mit ähnlichen Bedeutungen ähnlich dargestellt, wobei der Kontext der Wörter berücksichtigt wird. Word Embeddings repräsentieren Wörter als Vektoren in einem multidimensionalen semantischen Raum. In diesem Raum werden Wörter, die ähnlich zueinander sind, näher beieinander platziert. Diesen Raum kann man sich etwa folgendermaßen vorstellen:<br>\n",
    "\n",
    "\n",
    "![wordembeddings](https://miro.medium.com/max/1291/1*Fat62b1ZITOFMPXTcHNkLw.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die grundsätzliche Idee von Word Embeddings basiert auf der **Distributionellen Hypothese** von  John Rupert Firth, die besagt, dass die Bedeutung eines Wortes durch sein Umfeld geprägt ist. Wörter, die einen ähnlichen Kontext besitzen, haben eine ähnliche Bedeutung. Anders als vorherige Vektorisierungsmethoden basieren Word Embeddings auf **Vorhersage-Modellen** (engl.: prediction models), indem Wörter durch Wahrscheinlichkeiten anstatt Häufigkeiten wie beim Bag-of-Words Modell oder bei der Latent Semantic Analysis dargestellt werden. Weiterhin sind die Wortvektoren von Word Embeddings anders als beim Bag-of-Words Modell **dichtbesetzt** (engl. **dense**) und haben weitaus weniger Dimensionen (100-800 Dimensionen anstatt 100000-1000000, je nach der Größe des Vokabulars). Dadurch haben die Wortvektoren eine viel geringere Größe, bieten trotzdem eine effizientere und komplexere Darstellung der Wörter.\n",
    "\n",
    "\n",
    "Word Embeddings wurde ab 2013 durch die Einführung des Algorithmus **Word2vec** populär, in den Jahren darauf folgten weitere Word Embedding Algorithmen wie GloVe, FastText, ELMo und BERT. In der folgenden Tabelle sind die wichtigsten Unterschiede der verschiedenen Embeddings zusammengefasst, genauere Erläuterungen der Embeddings befinden sich in den folgenden Kapiteln."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: ausfüllen & mehr, gucken wie anders dargestellt werden kann\n",
    "\n",
    "<table>\n",
    "    <th></th>\n",
    "    <th>Word2Vec</th>\n",
    "    <th>Glove</th>\n",
    "    <th>FastText</th>\n",
    "    <th>ELMo</th>\n",
    "    <th>BERT</th>\n",
    "    <tr>\n",
    "        <td>Entstehungsjahr</td>\n",
    "        <td>2013</td>\n",
    "        <td>2014</td>\n",
    "        <td>2016</td>\n",
    "        <td>2018</td>\n",
    "        <td>2018</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td>Out of vocabulary Fehler</td>\n",
    "        <td>ja</td>\n",
    "        <td>ja</td>\n",
    "        <td>nein</td>\n",
    "        <td>nein</td>\n",
    "        <td>nein</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td>Kontextsensitiv</td>\n",
    "        <td>nein</td>\n",
    "        <td>nein</td>\n",
    "        <td>nein</td>\n",
    "        <td>ja</td>\n",
    "        <td>ja</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    </tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Die Popularität von Word Embeddings ist vor allem **Word2Vec** geschuldet, welches 2013 von Tomas Mikolov und weiteren Mitgliedern von Google publiziert wurde. Word2Vec ist im eigentlichen Sinne kein alleinstehender Algorithmus, sondern besteht aus zwei Techniken: dem **Continous Bag of Words** (**CBOW**) und dem **Skip-gram Model**. **Continous Bag of Words** versucht die Wahrscheinlichkeit eines Wortes oder einer Gruppe von Wörtern anhand eines gegebenen Kontext vorauszusagen:\n",
    "\n",
    "![cbow](img/cbow.png)\n",
    "\n",
    "\n",
    "Das **Skip-gram Model** funktioniert wie CBOW, nur anders herum. Das Modell versucht, anhand eines gegebenen Wortes den Kontext vorauszusagen:\n",
    "\n",
    "![skip-gram](img/skipgram.png)\n",
    "\n",
    "<br>Word2Vec nutzt wahlweise eine dieser Techniken, um aus rohen Textdaten mithilfe eines Neuronales Netzes Wortvektoren zu erstellen. Dabei kann Word2Vec bei vielen Implementierungen (z.b. bei Gensim) auf zwei Arten verwendet werden: Entweder werden bereits vortrainierte Embeddings geladen oder es werden eigene Embeddings auf eigenen Textdaten trainiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vortrainierte Embeddings verwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_w2v = gensim.models.KeyedVectors.load_word2vec_format('german_model.bin', \n",
    "                                                                 binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Länge des word embedding Vektors:  300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.1141477 , 0.11475459, 0.10349818, 0.29905704], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Länge des word embedding Vektors: \", len(pretrained_w2v['Koenig']))\n",
    "pretrained_w2v['Koenig'][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Prinz', 0.7856332659721375),\n",
       " ('Koenigs', 0.7356206774711609),\n",
       " ('Koenigin', 0.7255839109420776),\n",
       " ('Jungkoenig', 0.7053836584091187),\n",
       " ('Kaiser', 0.7047818303108215),\n",
       " ('Prinzen', 0.6925939321517944),\n",
       " ('Jungschuetzenkoenig', 0.6864457726478577),\n",
       " ('Kronprinz_Philippe', 0.6853584051132202),\n",
       " ('Koenig_Willem', 0.6849293112754822),\n",
       " ('Thron', 0.6848670840263367),\n",
       " ('Willem-_Alexander', 0.6764872074127197),\n",
       " ('Prinzessin', 0.6718595027923584)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_w2v.most_similar('Koenig', topn=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: hier weiter\n",
    "hier gucken https://github.com/fotisj/word_embeddings/blob/master/w2v_intro.ipynb\n",
    "\n",
    "plot_word_embeddings benutzen, um graphisch zu zeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigene Embeddings verwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 56s, sys: 3.28 s, total: 5min 59s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word2vec_cbow = Word2Vec(texts, min_count=1, size=100, window=5, sg=0)\n",
    "word2vec_skipgram = Word2Vec(texts, min_count=1, size=100, window=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.wv.most_similar('smartphone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- Vorwissen?\n",
    "- Unterschied BOW: \n",
    "    - dense\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMo\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "\n",
    "BERT steht für \"Bidirectional Encoder Representations from Transformers\" und brach bei seiner Veröffentlichung 2018 eine ganze Reihe von Rekorden für Aufgaben des Natural Language Processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Inhaltsverzeichnis",
   "title_sidebar": "Inhalte",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
