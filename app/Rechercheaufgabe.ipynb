{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOS:<br>\n",
    "\n",
    "- Übersicht über alle Verfahren geben in kapitel 3 (Tabelle?!)\n",
    "- überprüfen Formulierung\n",
    "- untertitel der grafik bei LSA in pdf überprüfen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Inhaltsverzeichnis<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Einführung\" data-toc-modified-id=\"Einführung-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Einführung</a></span><ul class=\"toc-item\"><li><span><a href=\"#Das-Bag-of-Words-Modell\" data-toc-modified-id=\"Das-Bag-of-Words-Modell-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Das Bag-of-Words Modell</a></span></li><li><span><a href=\"#Grenzen-des-Bag-of-Words-Modells\" data-toc-modified-id=\"Grenzen-des-Bag-of-Words-Modells-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Grenzen des Bag-of-Words Modells</a></span></li></ul></li><li><span><a href=\"#LSA\" data-toc-modified-id=\"LSA-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>LSA</a></span></li><li><span><a href=\"#Word-Embeddings\" data-toc-modified-id=\"Word-Embeddings-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Word Embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#word2vec\" data-toc-modified-id=\"word2vec-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>word2vec</a></span></li><li><span><a href=\"#Glove\" data-toc-modified-id=\"Glove-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Glove</a></span></li><li><span><a href=\"#FastText\" data-toc-modified-id=\"FastText-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>FastText</a></span></li><li><span><a href=\"#ELMo\" data-toc-modified-id=\"ELMo-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>ELMo</a></span></li><li><span><a href=\"#BERT\" data-toc-modified-id=\"BERT-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>BERT</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einführung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das **Natural Language Processing** (kurz: NLP) befasst sich mit Methoden und Verfahren zur maschinellen Verarbeitung von natürlicher Sprache in Form von Worten, Texten oder ganzen Korpora. Bevor jedoch NLP Verfahren wie die Textklassifikation oder das Topic Modelling auf die Textdaten angewendet werden können, müssen diese in eine Darstellungsweise umgewandelt werden, mit der die Verfahren arbeiten können. Die rohen Textdaten werden daher in **Vektoren**, die aus Zahlen bestehen, umgewandelt. Dieser Vorgang nennt sich **Vektorisierung**. Ein Wort wie \"Baum\" kann dadurch als Vektor aufgefasst werden. Natürlich können auch andere Features aus den Texten als Vektoren dargestellt werden; so ist es auch möglich, einzelne Buchstaben, Phrasen, Sätze, Segmente oder ganze Texte als Features aus den Textdaten zu extrahieren und diese zu vektorisieren. In der folgenden Übersicht werden jedoch vorwiegend Wörter als Features verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das Bag-of-Words Modell\n",
    "\n",
    "Das wohl einfachste Verfahren zur Darstellung von Wörtern als Vektoren ist das **Bag-of-Words** Modell. Wörter werden hier als eindimensionale Vektoren (= einfache Zahl) dargestellt, wobei jedes individuelle Wort einen individuellen eindimensionalen Vektor (auch: **Index**) zugeordnet bekommt. Die Zuordnungen jedes einzigartigen Wortes zu seinem Vektor werden in einem *Vokabular* gespeichert. Nun können mithilfe dieses Vokabulars auch ganze Sätze oder sogar Texte dargestellt werden. Dafür wird für jeden Satz/Text ein Vektor gebildet, der die gleiche Länge wie das Vokabular hat. Jedem Eintrag des Vektors wird anhand des Vokabulars ein Wort zugeordnet. Der Satz/Text wird dann als Vektor aus **absoluten Termhäufigkeiten** dargestellt, wo an jeder Stelle, an dem ein Wort aus dem Vokabular in dem Text vorkommt, die Häufigkeit des Wortes in dem jeweiligen Satz/Text steht und an jeder anderen Stelle eine $0$, da es kein einziges Mal vorkommt.[<sup>1</sup>](#fn1) Dies soll im Folgenden anhand eines Code-Beispiels erläutert werden. Zuerst wird das Vokabular aller Texte dargestellt, bei dem die Wörter einem Index zugeordnet werden (es wird ab $0$ gezählt). Danach werden die vektorisierten Sätze/Texte angezeigt.\n",
    "\n",
    "<hr style=\"border: 0.1px solid black;\"/>\n",
    "<div id=\"fn1\" style=\"font-size:8pt; line-height:1; padding-left: 1em; text-indent: -1em\"><sup style=\"font-size:5pt\">1</sup> &nbsp; Dies ist nur eine Möglichkeit, die Häufigkeit eines Wortes beim Bag-of-Words Modell darzustellen. Eine weitere Möglichkeit wären <b>binäre Häufigkeiten</b>, bei denen das Vorkommen eines Wortes mit einer $1$ und die Abwesenheit eines Wortes mit einer $0$ gekennzeichnet werden. Um häufigen Wörtern in den Dokumenten weniger Gewicht zu geben, da dies oft einen geringeren Informationsgehalt besitzen, ist es auch möglich, das Bag-of-Words Modell in der Kombination mit dem <b>TF-IDF Maß</b> aus dem Bereich des Information Retrievals zu verwenden, bei dem die Häufigkeit von Worten skaliert wird.</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ich': 10, 'gehe': 6, 'nachher': 16, 'zur': 24, 'bank': 1, 'um': 21, 'etwas': 5, 'geld': 7, 'zu': 23, 'holen': 8, 'möchte': 15, 'mich': 13, 'kurz': 11, 'auf': 0, 'die': 3, 'setzen': 19, 'mir': 14, 'essen': 4, 'stand': 20, 'von': 22, 'der': 2, 'hölzernen': 9, 'neben': 17, 'liegt': 12, 'noch': 18}\n"
     ]
    }
   ],
   "source": [
    "text = [\"ich gehe nachher zur bank, um etwas geld zu holen\",\n",
    "        \"ich möchte mich kurz auf die bank setzen\",\n",
    "        \"um mir etwas zu essen zu holen, stand ich von der bank auf\", \n",
    "        \"auf der hölzernen bank neben der bank liegt noch geld\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vector = vectorizer.fit_transform(text)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1]\n",
      " [1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0]\n",
      " [1 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 2 0]\n",
      " [1 2 2 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grenzen des Bag-of-Words Modells\n",
    "\n",
    "Aufgrund seiner Einfachheit ist das Bag-of-Words Modell gut verständlich und sehr schnell umsetzbar. Es hat jedoch eine Reihe an Nachteilen, von deinen einige im Folgenden kurz erläutert werden:\n",
    "\n",
    "- **Keine Informationen über Reihenfolge der Wörter.** Beim Bag-of-Words Modell wird jegliche Information über die Reihenfolge der Wörter verworfen, der Kontext eines Wortes bleibt unberücksichtigt. Dies wird auch durch den Namen dieses Modells deutlich: Die Bezeichnung \"bag\" (deutsch: Sack) soll darauf hinweisen, dass alle Informationen über die Struktur oder Reihenfolge der Wörter im Dokument verworfen werden, da sie metaphorisch in einen \"Sack\" geworfen werden. Die Reihenfolge lässt sich auch nicht im Nachhinein rekonstruieren. Insgesamt gehen somit sehr viele semantische Informationen verloren. Eine Lösung, bei der die Reihenfolge der Worte berücksichtigt werden kann, ist die Verwendung von **N-Grammen** oder **LSA** (siehe Kapitel 2 TODO). \n",
    "- **Spärlichkeit von Wortvektoren.** Umso mehr verschiedene Worte in den verwendeten Texten vorkommen, umso größer wird das Vokabular. Dies kann oft zu sehr spärlichen (engl. *sparse*) Wortvektoren führen. Besteht das Vokabular aus 500000 Worten, ein Text aber nur aus 50 verschiedenen Worten, sind nur 0.01% der Stellen des 500000 langen Wortvektors mit Einsen besetzt, der Rest nur mit Nullen. Dies führt dazu, dass eine große Menge an Rechenspeicher für die Verarbeitung der riesigen Matrizen benötigt wird. Weiterhin werden wenige Informationen in sehr großen Repräsentationsräumen benutzt, wodurch es für einige NLP Verfahren und Modelle problematisch ist, diese wenigen Information effizient zu nutzen. Eine Lösung bieten dichtbesetzte **Word Embeddings**, die in den nächsten Kapiteln behandelt werden.\n",
    "- **Abbildung der Mehrdeutigkeit von Worten**. Wörter können trotz gleicher Schreibweise mehrere Bedeutungen haben, welche sich durch den Kontext des Wortes zeigen können. Dies wird durch das Bag-of-Words Modell nicht abgebildet. Eine mögliche Lösung wäre die Verwendung von **kontextabhängigen Word Embeddings** wie die **BERT-Embeddings** in Kapitel 7 (TODO)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA\n",
    "\n",
    "**Latent Semantic Analysis** (kurz: LSA, auch: *Latent Semantic Indexing*) ist ein Verfahren aus dem Bereich des Information Retrievals aus dem Jahre 1990. Bei diesem Verfahren werden Dokumente und Terme (repräsentiert durch eine **Term-Dokument Matrix**) in einem latenten Raum abgebildet, der aus **Konzepten** (oder **Hauptkomponenten**) besteht. Dokumente, die ähnlich zueinander sind, d.h. aus ähnlichen Konzepten bestehen, werden in diesem Raum näher beieinander platziert. Dies wird durch die folgende Grafik deutlich.\n",
    "\n",
    "\n",
    "![lsa](img/lsa.png)\n",
    "\n",
    "Grafik von Susan Dumais, siehe <a href=\"http://www.ifis.uni-luebeck.de/~moeller/tuhh-lectures/mmieir-sose-12/05-Latent-Semantic-Analysis.pdf\">Präsentation</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ziel der LSA ist es, die Konzepte innerhalb der Dokumente zu finden. Dabei greift das Verfahren auf eine Technik der linearen Algebra zurück, der **Singulärwertzerlegung** (englisch: Singular Value Decomposition). Die Idee dabei ist, dass die Term-Dokument Matrix aus **Hauptdimensionen**, welche die wichtigen Konzepte der Dokumente beinhalten, und aus weniger aussagekräftigen Dimensionen mit unwichtigen Termen besteht. Mithilfe der Singulärwertzerlegung wird die originale Term-Dokument Matrix in drei Matrizen aufgeteilt, wobei die beiden äußeren Matrizen aus den linken bzw. rechten orthonormalen Eigenvektoren bestehen und die mittlere Matrix eine Diagonalmatrix ist, die die singulären Werte der Originalmatrix enthält. Mit Hilfe dieser Zerlegung kann eine **Approximation** der Originalmatrix mit einer kleiner dimensionierten Matrix erreicht werden. Die singulären Werte in der Diagonalmatrix sind nach ihrer Größe absteigend geordnet. Singulärwerte, die unter einem bestimmten Schwellenwert liegen, werden entfernt. Auch in den anderen Matrizen werden entsprechende Zeilen oder Spalten entfernt. Mit Hilfe der reduzierten Matrizen erhält man durch Matrixmultiplikation die optimale Approximation der Originalmatrix, die kleiner als die originale Term-Dokument Matrix ist, da Informationen aus den weniger aussagekräftigen Dimensionen verworfen wurden. Weiterhin werden bei der Dimensionsreduktion auch ähnliche Konzepte zusammengefasst, so werden z.B. Worte wie \"Tür\" und \"Tor\" in einem Konzept zusammengefasst.\n",
    "\n",
    "Der Vorteil der LSA ist, dass anders als beim Bag-of-Words Modell die **Semantik** der Dokumente widergegeben werden kann. Zudem werden **Synonyme** zusammengefasst. Probleme hat LSA jedoch mit der **Polysemie**. Zudem ist der Algorithmus sehr rechenaufwendig.\n",
    "\n",
    "TODO: mehr?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "**Word Embeddings** (deutsch: Worteinbettungen) sind die Sammelbezeichnung für eine Reihe von Sprachmodellierungstechniken. Anders als beim Bag-of-Words Modell werden Wörter mit ähnlichen Bedeutungen ähnlich dargestellt, wobei der Kontext der Wörter berücksichtigt wird. Word Embeddings repräsentieren Wörter als Vektoren in einem multidimensionalen semantischen Raum. In diesem Raum werden Wörter, die ähnlich zueinander sind, näher beieinander platziert. Diesen Raum kann man sich etwa folgendermaßen vorstellen:<br>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1291/1*Fat62b1ZITOFMPXTcHNkLw.jpeg\" alt=\"Word Embedding Space\" width=\"600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die grundsätzliche Idee von Word Embeddings basiert auf der **Distributionellen Hypothese** von  John Rupert Firth, die besagt, dass die Bedeutung eines Wortes durch sein Umfeld geprägt ist. Wörter, die einen ähnlichen Kontext besitzen, haben eine ähnliche Bedeutung. Word Embeddings wurde ab 2013 durch die Einführung des Algorithmus **word2vec** populär, in den folgenden Jahren folgten weitere Word Embedding Algorithmen wie GloVe, FastText, ELMo und BERT. In der folgenden Tabelle sind die wichtigsten Unterschiede der verschiedenen Embeddings zusammengefasst, genauere Erläuterungen der Embeddings befinden sich in den folgenden Kapiteln."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: ausfüllen & mehr\n",
    "\n",
    "<table>\n",
    "    <th></th>\n",
    "    <th>word2vec</th>\n",
    "    <th>Glove</th>\n",
    "    <th>FastText</th>\n",
    "    <th>ELMo</th>\n",
    "    <th>BERT</th>\n",
    "    <tr>\n",
    "        <td>Entstehungsjahr</td>\n",
    "        <td>2013</td>\n",
    "        <td>2014</td>\n",
    "        <td>2016</td>\n",
    "        <td>2018</td>\n",
    "        <td>2018</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td>Out of vocabulary Fehler</td>\n",
    "        <td>ja</td>\n",
    "        <td>ja</td>\n",
    "        <td>nein</td>\n",
    "        <td>nein</td>\n",
    "        <td>nein</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td>Kontextsensitiv</td>\n",
    "        <td>nein</td>\n",
    "        <td>nein</td>\n",
    "        <td>nein</td>\n",
    "        <td>ja</td>\n",
    "        <td>ja</td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    </tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMo\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Inhaltsverzeichnis",
   "title_sidebar": "Inhalte",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
