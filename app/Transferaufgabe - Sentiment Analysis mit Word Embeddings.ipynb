{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Inhaltsverzeichnis<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Einleitung\" data-toc-modified-id=\"Einleitung-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Einleitung</a></span></li><li><span><a href=\"#Das-Korpus\" data-toc-modified-id=\"Das-Korpus-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Das Korpus</a></span></li><li><span><a href=\"#Theoretische-Grundlagen\" data-toc-modified-id=\"Theoretische-Grundlagen-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Theoretische Grundlagen</a></span><ul class=\"toc-item\"><li><span><a href=\"#Word-Embeddings\" data-toc-modified-id=\"Word-Embeddings-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Word Embeddings</a></span></li><li><span><a href=\"#Convolutional-Neural-Networks\" data-toc-modified-id=\"Convolutional-Neural-Networks-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Convolutional Neural Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Allgemeine-Funktionsweise\" data-toc-modified-id=\"Allgemeine-Funktionsweise-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Allgemeine Funktionsweise</a></span></li><li><span><a href=\"#KimCNN\" data-toc-modified-id=\"KimCNN-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>KimCNN</a></span></li></ul></li><li><span><a href=\"#Maschinelle-Lernverfahren\" data-toc-modified-id=\"Maschinelle-Lernverfahren-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Maschinelle Lernverfahren</a></span></li></ul></li><li><span><a href=\"#Experimente\" data-toc-modified-id=\"Experimente-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Experimente</a></span><ul class=\"toc-item\"><li><span><a href=\"#Aufbau\" data-toc-modified-id=\"Aufbau-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Aufbau</a></span></li><li><span><a href=\"#Ergebnisse\" data-toc-modified-id=\"Ergebnisse-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Ergebnisse</a></span></li></ul></li><li><span><a href=\"#Schlussbetrachtung\" data-toc-modified-id=\"Schlussbetrachtung-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Schlussbetrachtung</a></span></li><li><span><a href=\"#Literaturverzeichnis\" data-toc-modified-id=\"Literaturverzeichnis-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Literaturverzeichnis</a></span></li><li><span><a href=\"#Appendix\" data-toc-modified-id=\"Appendix-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Appendix</a></span><ul class=\"toc-item\"><li><span><a href=\"#Daten-und-Code\" data-toc-modified-id=\"Daten-und-Code-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Daten und Code</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einleitung\n",
    "\n",
    "Die **Sentiment Analysis** ist ein wissenschaftliches Feld des Natural Language Processing, welches sich sich mit Texten befasst, die Meinungen, Stimmungen, Einschätzungen und Emotionen von Menschen beinhalten (Liu 2015, S. 1). Dazu gehören z.B. Filmkritiken, Produktreviews oder Twitterposts. In dieser Arbeit wird die Sentiment Analysis als eine besondere Form der Textklassifikation angesehen. Wichtig bei der Sentiment Analysis sind vor allem Schlüsselwörter oder -phrasen, die Auskunft über die Meinung, Stimmung oder Emotion des Textes geben. In früheren Jahren wurden dafür zu Unterstützung der Textklassifikationstechniken sogenannte \"Stimmungslexika\" verwendet, die den entsprechenden Wörter/Phrasen eine Stimmung (z.B. \"gut\", \"schlecht\", \"neutral\") zuordneten (Liu 2015, S. 10f.). Dadurch konnten jedoch Probleme wie die sich ändernde Semantik eines Wortes hinsichtlich des Kontextes nicht gelöst werden  (Liu 2015, S. 10f.). In den letzten Jahren wurden daher immer häufiger die sich als sehr effektiv erweisenden **Word Embeddings** im Rahmen der Sentiment Analysis verwendet (Petrolito, Dell'Orletta 2019, S. 330), da sie beim Erstellen der Wortrepräsentation den Kontext eines Wortes berücksichtigen.\n",
    "\n",
    "\n",
    "In dieser Arbeit wurden eine Sentiment Analysis anhand eines Korpus von Nutzerreviews des Onlineversandhändlers **Amazon** in der Produktkategorie \"Elektronik\" durchgeführt. Für die Sentiment Analysis wurden mehrere Klassifizierungstechniken verwendet. Die zentrale Technik war ein Convolutional Neural Network (CNN), welches in Kombination mit vortrainierten Word Embeddings verwendet wurde. Als CNN-Architektur wurde das CNN von Yoon Kim verwendet, welches mit einer einfachen Neuronalen Netz Architektur seinerzeit mehrere State-of-the-art Klassifizierungsansätze übertreffen konnte, darunter auch einen Sentiment Analysis Datensatz (Kim, 2014). Kim verwendete in seinen Experimenten die Word2Vec Embeddings, in dieser Arbeit wurden stattdessen die neueren **GloVe** und **FastText** Embeddings verwendet. Während Kim nur ein votrainiertes Embeddingmodell benutzte, sollten in dieser Arbeit insgesamt fünf verschiedene Embeddingmodelle in Kombination mit dem CNN von Kim verwendet werden (zwei FastText- und drei GloVe Embeddingmodelle). Es sollte untersucht werden, welches der fünf Embeddingmodelle sich am besten für die Sentiment Analysis des Nutzerreview-Korpus eignete. Diese Ergebnisse wurden mit zwei weiteren Ansätzen verglichen: einer Machine Learning Klassifikation und einem *fine-tuned* BERT-Modell. Anders als CNNs sind Support Vector Machines (SVM) und Logistic Regression keine neuronalen Netze, sondern gehören zu den maschinellen Lernverfahren und werden bereits seit Jahrzehnten für die Textklassifikation verwendet. Sie nutzen jedoch keine vortrainierten Gewichte von Word Embeddings, sondern lernen \"from scratch\", d.h. alle Informationen, die zur Klassifizierung  verwendet werden, werden lediglich aus den Trainingsdaten gewonnen. Es sollte deshalb untersucht werden, ob die Nutzung von Word Embeddings einen Vorteil für die Sentiment Analysis Experimente bietet oder ob bereits ein einfaches Machine Learning Verfahren wie SVM oder Logistic Regression ohne vortrainierte Gewichte ähnliche Resultate wie das CNN inklusive Word Embeddings erreichen konnte. Zuletzt sollten die Ergebnisse der CNNs und der Machine Learning Verfahren mit einem *fine-tuned* BERT-Modell verglichen werden. BERT wurde Ende 2018 von Devlin et. al. veröffentlicht und konnte zu der Zeit mehrere State-of-the-art Ergebnisse in vielen Aufgaben des Natural Language Processing erzielen (Devlin et. al., 2018). Auch fast zwei Jahre nach seiner Veröffentlichung ist BERT noch sehr populär und wird bei vielen Aufgaben des Natural Language Processing verwendet. Wie das hier verwendete CNN benutzt BERT für die Klassifizierung vortrainierte Gewichte, die es aus eigenen bereitgestellten Embeddings extrahiert. Diese BERT-Embeddings unterscheiden sich jedoch von den GloVe- und FastText-Embeddings, da sie dynamisch den Kontext eines Wortes berücksichtigen und somit Worte mit gleicher Schreibweise aber unterschiedlichen Bedeutungen voneinander unterscheiden können. BERT belegt bei einer Auflistung der State-of-the-art Verfahren für die Sentiment-Analysis des gesamten Amazon-Review-Korpus aktuell den ersten Platz.(FN: Siehe https://paperswithcode.com/sota/sentiment-analysis-on-amazon-review-full (abgerufen am 23.08.2020).) An zweiter Stelle des Rankings befindet sich jedoch ein CNN. Es sollte deshalb untersucht werden, ob das neuere und komplexere BERT-Modell, welches weitaus hochdimensionalere Embeddings als GloVe und FastText verwendet, bessere Ergebnisse als die simple CNN Architektur von Kim erzielen konnte oder ob dieses oder eines der Machine Learning Verfahren für die Sentiment Analysis des Reviewkorpus in der ausgewählten Kategorie \"Elektronik\" bessere Genauigkeiten erzielen konnten. Das primäre Vergleichskriterium der drei Verfahren war deshalb die Klassifizierungsgenauigkeit, weitere Faktoren waren die Dauer des Trainings, die Dauer der Hyperparameteroptimierung und die benötigte Rechenleistung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Das Korpus\n",
    "\n",
    "Das verwendete Korpus ist ein Sammlung von englischsprachigen Nutzerreviews zu den Produkten des Onlineversandhändlers **Amazon** von Julian McAuley ([Quelle](https://nijianmo.github.io/amazon/index.html)). Der Zeitraum der Veröffentlichungsdaten der Reviews im originalen Korpus liegt zwischen dem Mai 1996 und dem Oktober 2018. Diese Zeitspanne umfasst ~233 Millionen Reviews aus 29 verschiedenen Produktkategorien. Zu jedem Produkt stehen die Bewertung in einer Skala von 1 bis 5 (sehr schlecht bis sehr gut) zur Verfügung, der Reviewtext, die Anzahl der \"Nützlich\"-Votierungen, eine Verifizierung von Amazon, die Produkt-Metadaten und weitere Links. \n",
    "\n",
    "Für diese Arbeit wurde eine verkürzte Version des Korpus verwendet. Alle Produktreviews stammen aus der Kategorie \"Elektronik\" und aus dem Jahr 2018. Es wurden nur Reviews berücksichtigt, die zu jeder ausgewählten Metainformation (\"Bewertung\", \"Nutzername\", \"Reviewtext\", \"Verifizierung\", \"Datum\") Werte enthielten. Zudem wurden die Zusammenfassung und der eigentliche Text eines Reviews zusammengeführt. Das Bewertungssystem wurde für diese Arbeit angepasst und von fünf Sterne auf drei Sterne reduziert. Bewertungssysteme mit fünf Sternen sind problematisch, da diese oft eine bimodale Verteilung hinsichtlich der Extremwertungen $1$ und $5$ aufweisen. (FN: Siehe http://www.lifewithalacrity.com/2006/08/using_5star_rat.html (abgerufen am 23.08.2020).) Vor allem die $2$ und $4$ Sterne-Bewertungen lassen sich nicht immer ganz offensichtlich von den $1$ und $5$ Sternen abgrenzen, da Nutzer für sich selbst individuelle Bewertungsrichtlinien festlegen. Für diese Arbeit wurden deshalb die $2$ und $4$ Sterne Bewertungen mit den $1$ und $5$ Sterne Bewertungen zusammengeführt, sodass es nur noch drei Bewertungseinheiten gab (siehe Korpusausschnitt in Tabelle 1): **positiv** ($4$ und $5$ Sterne), **neutral** ($3$ Sterne) und **negativ** ($1$ und $2$ Sterne). Das resultiernde Korpus zeigte hinsichtlich der Klassenverteilung eine starke Unausgeglichenheit, weshalb mithilfe von zufälligem Downsampling zu jeder Klasse 15000 Nutzerreviews ausgewählt wurden, um ein ausgeglichenes Korpus zu erhalten (Gesamtgröße: 45000 Reviews). Zusätzlich wurde die Spalte \"length\" hinzugefügt, welche die Länge der Reviews beinhaltet. \n",
    "\n",
    "|       | rating   | name        | review                              | verified   |   vote | date       |   length |\n",
    "|------:|:---------|:------------|:------------------------------------|:-----------|-------:|:-----------|---------:|\n",
    "| 34664 | positive | leo felix   | great deal                          | True       |      0 | 18.01.2018 |        2 |\n",
    "| 17394 | neutral  | Benjiboi666 | does not work on xbox or ps         | True       |      0 | 21.01.2018 |        7 |\n",
    "| 11339 | negative | Lost-creek  | Stops recording after a few minutes | True       |      0 | 06.01.2018 |        6 |\n",
    "\n",
    "**Tabelle 1**: Das Amazon-Nutzerreview-Korpus in der Kategorie \"Elektronik\" von 2018 (Ausschnitt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretische Grundlagen\n",
    "\n",
    "## Word Embeddings\n",
    "\n",
    "**Word Embeddings** sind eine besondere Art der distributiven Repräsentation von Wörtern (PILHEVAR 2020, S. 27). Sie bauen auf der Idee der **Distributionellen Hypothese** von John Rupert Firth auf, die besagt, dass die Bedeutung eines Wortes durch sein Umfeld geprägt ist. Wörter, die einen ähnlichen Kontext besitzen, haben eine ähnliche Bedeutung. Word Embeddings konstruieren diese Wortrepräsentationen mithilfe von Neuronalen Netzen und basieren meist auf Sprachmodellierungstechniken, mithilfe derer nachfolgende oder fehlende Wörter vorausgesagt werden können. In dieser Arbeit wurden die Word Embeddings **GloVe**, **FastText** und **BERT** verwendet. **GloVe** wurde 2014 von Pennigton et. al. veröffentlicht (Pennigton u.a. 2014). Anders als andere Word Embedding Verfahren verwendet GloVe für die Darstellung der Worthäufigkeiten keine Voraussagemodelle in Form von neuronalen Netzen, sondern eine Kookkurrenz-Matrix, die mithilfe einer Mischung aus maschinellem Lernen und statischen Verfahren aus den Texten gewonnen wird. GloVe hat den Nachteil, dass es nicht gut mit unbekannten Wörtern arbeiten kann (= *Out of vocabulary*-Fehler). Ein Verfahren, welches dieses Problem umgeht, ist das 2016 von Bojanowski et. al. veröffentlichte **FastText** (Bojanowski u.a. 2016). FastText löst das OOV-Problem, indem es während des Trainings anstatt ganzer Wörter Buchstaben-N-Gramme lernt, aus denen unbekannte Wörter zusammengebaut werden können. Dies ist leider keine optimale Lösung, da Wörter zwar aus ähnlichen Buchstaben N-Gramm-Bestandteilen bestehen, sich aber semantisch trotzdem stark voneinander unterscheiden können. Eine bessere Lösung des OOV-Problems bietet das 2018 von Devlin et. al. veröffentliche **BERT** (Devlin u.a. 2018). Wie FastText auch lernt BERT keine ganzen Wörter, sondern Teilwörter, aus welchen es unbekannte Wörter zusammenbauen kann. Anders als FastText oder GloVe zählt BERT jedoch zu den *contextualised Word Embeddings*, was bedeutet, dass es den Kontext eines Wortes bei der Bildung des Embeddings berücksichtigt. Dies erreicht BERT durch den sogenannten **Attention**-Mechanismus des **Transformers**-Modell, der es erlaubt, relevanten Worten in einer Sequenz mehr Bedeutung als anderen Worten zuzuschreiben. Dabei betrachtet BERT vorhergehende und nachfolgende Wörter (unidirektionaler Ansatz). Da sich durch diesen Ansatz Wörter jedoch \"selber sehen\" können, verwendet BERT zusätzlich noch die Konzepte **Next Sentence Prediction** (NSP) und **Masked Language Modeling** (MLM). Bei der Next Sentence Prediction überprüft BERT, ob der aktuell betrachtete Satz kontextuell zum nachfolgenden Satz passt. Beim Masked Language Modeling maskiert BERT nach einer gewissen Strategie Wörter, um diese mithilfe der umliegenden Wörter voraussagen zu können. Somit lernt BERT den Kontext von Wörtern, was es BERT erlaubt, zwischen mehrdeutigen Wörtern zu unterscheiden. Ein weiterer Unterschied von BERT zu GloVe und FastText ist, dass es keine **statische**, sondern eine **dynamische** Repräsentation der Wörter liefert. Worte, die die gleiche Schreibweise besitzen, können somit durch unterschiedliche Vektoren dargestellt werden, je nach Kontext und Reihenfolge. Dies bedeutet aber auch, dass auch nach dem Training des Modells dieses für die Benutzung der Embeddings obligatorisch ist. Bei den statischen Word Embeddings GloVe und FastText werden lediglich die Embeddings in Form von Wortvektoren benötigt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "### Allgemeine Funktionsweise\n",
    "\n",
    "**Convolutional Neural Networks** (CNN) sind eine bestimmte Form von neuronalen Netzen, die vorwiegend für die Klassifizierung von Bildern verwendet werden. Anders als Feedforward Netze lernt ein CNN keine globalen Muster in den Daten, sondern lokale Muster (Chollet, 2016, S. 122). Somit werden keine zufälligen, sondern aufeinanderfolgende und umliegende Merkmalskombinationen gelernt. Bei einem Bild sind das Ausschnitte der Bilder, die z.B. mithilfe eines quadratischen 3x3 Filters erzeugt werden. Das Lernen der Merkmalskombinationen geschieht in den namensgebenden **Convolutional Layern**, die vor den Dense Layern eines Neuronalen Netzes angefügt werden (Géron, 2020, S. 451). Mithilfe der Convolutional Layer ist ein CNN in der Lage, wiederkehrende Muster an verschiedensten Stellen des Bildes zu erkennen. Weiterhin erlaubt eine Aneinandereihung mehrerer Convolutional Layer das Erkennen komplexerer und abstrakterer Merkmale, wobei zu Beginn eines Convolutional Layers eher kleinere Muster wie Kanten erkannt werden (Chollet, 2018, S. 123). \n",
    "\n",
    "In den Convolutional Layern wird ein **Filter** (oder: Kernel) auf die Featurematrix angewandt, woraus eine **Feature Map** (auch: Activation Map) entsteht. Dies wird durch Abbildung 1 deutlich: Auf jedes mögliche Feature eines Eingabebilds, welches aus 25 (5x5) Pixeln bzw. Featuren besteht, wird ein 3x3 Filter angewandt. Dies ist bei allen Featuren außer den Features am Rand möglich, da dort der Filter nicht vollends angewandt werden kann. Das Ergebnis für jede Anwendung des Filters auf möglichen Features der Eingabematrix ist ein Wert, der ein *Neuron* bzw. ein *gelerntes Feature* darstellt (Weidman, 2020, S. 130). Mathematisch wird der Wert mit $w^T \\cdot x + b$ berechnet, wobei $w$ die zufällig initialisierte Filtermatrix, $x$ die Eingabematrix und $b$ ein typischer Bias eines Neuronalen Netzes ist. Aus allen so berechneten Werten bzw. gelernten Features ergibt sich die **Feature Map**. Dieser ganze Vorgang wird **Convolution** (deutsch: Faltung) genannt und ist die Kernoperation jedes CNNs.\n",
    "\n",
    "![feature_mapping](img/feature_mapping.png)\n",
    "\n",
    "**Abbildung 1**. Die Abbildung wurde aus folgendem Artikel entnommen: [An Intuitive Explanation of Convolutional Neural Networks](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/).\n",
    "\n",
    "Da jeder Filter in der Eingabematrix ein bestimmtes Muster oder Konzept erkennt, werden mehrere Filter in einem Convolutional Layer verwendet. Je mehr Filter verwendet werden, desto mehr Features können aus den Eingabedaten extrahiert werden (Karn, 2016). Auf jede Ausgabe eines Convolutional Layer wird eine Aktivierungsfunktion wie z.B. die ReLU-Funktion angewandt. Bevor die Ausgabe eines vorangegangenen Convolution Layers an das nächste Convolution oder Dense Layer übergeben wird, wird sie oft einem **Pooling Layer** übergeben. Die Aufgabe eines Pooling Layers ist die Verkleinerung des Bildes mithilfe von Pooling-Filtern, um die Anzahl der Parameter, die Rechenlast und das Risiko von Overfitting zu verringern (Géron, 2020, S. 460). Es gibt verschiedene Arten von Pooling, das in dieser Arbeit verwendete Pooling ist das Max-Pooling, bei dem nur der größte Wert des Pooling Filters dem folgenden Layer übergeben wird.\n",
    "\n",
    "Es ist auch möglich, CNNs für andere Dateitypen zu verwenden. Da in dieser Arbeit Textdaten verwendet werden, muss die CNN Architektur für diese Daten angepasst werden. Anstatt zweidimensionaler Convolutional Layer benutzt man bei Textdaten oft eindimensionale Convolutional Layer (Géron, 2020, S. 524). Beim eindimensionalen Convolutional Layer werden viele Filter über eine Textsequenz geschoben, womit für jeden Filter eine eindimensionale Feature Map erzeugt wird. Somit lernt das Netz bei jedem Filter ein einzelnes, sequenzielles Muster und die Filter können in Kombination mit Max-Pooling relevante N-Gramme entdecken (JACOVI, 2018, S. 56). Damit können eindimensionale CNNs eine schnellere Alternative zu Rekurrenten Neuronalen Netzen für simple Aufgaben wie die Textklassifikation sein (CHOLLET, 2018, S. 225). Laut Goldberg sind CNNs nützlich für Textklassikationsprobleme, bei denen es wichtige, lokale Hinweise in den Daten hinsichtlich der Klassenzugehörigkeit gibt, die jedoch an verschiedenen Stellen eines Dokuments auftauchen (Goldberg, 2015, S. 348). Damit eignen sich CNNs für die Entdeckung von relevanten, positionsunabhängigen Wortfolgen, welches bei der Sentiment Analysis z.B. aussagekräftige Wörter oder Phrasen sein können, die Bedeutungsträger von Meinungen, Stimmungen, Einschätzungen und Emotionen sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KimCNN\n",
    "\n",
    "2014 veröffentlichte Yoon Kim das Paper \"Convolutional Neural Networks for Sentence Classification\", in welchem er eine CNN Modellarchitektur vorstellte (Kim, 2014), welches im Verlauf dieser Arbeit \"KimCNN\" genannt wird. Zu dem Zeitpunkt konnte KimCNN in Kombination mit Word2Vec Embeddings State-of-the-art Ergebnisse im Bereich der Textklassifikation bei 4 von 7 getesteten Datensätzen erreichen, darunter auch bei einem Sentiment Analysis Datensatz (Kim, 2014, S. 1746). In dieser Arbeit wurde das Modell in Kombination mit den Word2Vec-Nachfolgern **GloVe** und **FastText** verwendet. Zudem wurde nur die Version \"CNN-non-static\" verwendet, bei welcher die vortrainierten Word Embeddings für die entsprechende Aufgabe gefinetuned werden. Bei KimCNN werden die Convolutions wortweise über die Eingabevektoren berechnet, wobei unterschiedlich große Filterfenster (Werte: 3, 4, 5) für die gleichzeitige Verarbeitung von Wörtern verwendet wurden. Die resultierenden Feature-Maps wurden mit einem Max-Pooling-Layer verarbeitet, um die extrahierten Features zu verdichten oder zusammenzufassen. Das Modell sieht wie folgt aus: \n",
    "\n",
    "```python\n",
    "KimCNN(\n",
    "  (embedding): Embedding(25002, 300, padding_idx=1)\n",
    "  (convs): ModuleList(\n",
    "    (0): Conv1d(1, 100, kernel_size=(3, 300), stride=(1,))\n",
    "    (1): Conv1d(1, 100, kernel_size=(4, 300), stride=(1,))\n",
    "    (2): Conv1d(1, 100, kernel_size=(5, 300), stride=(1,))\n",
    "  )\n",
    "  (fc): Linear(in_features=300, out_features=3, bias=True)\n",
    "  (dropout): Dropout(p=0.5, inplace=False)\n",
    ")\n",
    "```\n",
    "\n",
    "Die Werte bedeuten folgendes:\n",
    "- $25002$ (embedding): Größe des Vokabulars (= die maximale Anzahl an Features) plus ein Token für unbekannte Wörter `<UNK>` und ein Padding-Token `<PAD>`\n",
    "- $300$ (embedding): Dimension des Embeddings\n",
    "- $100$ (convs): Anzahl der Filter\n",
    "- $3, 4, 5$ (convs): Filtergrößen\n",
    "- $3$ (fc): Anzahl der Klassen (positiv, neutral, negativ)\n",
    "- $0.5$ (dropout): Dropoutrate\n",
    "\n",
    "KimCNN verwendete folgende Parameter, die mithilfe von GridSearch für alle von Kim getesteten Datensätze die besten Ergebnisse lieferten und auch für die Experimente in dieser Arbeit verwendet wurden:\n",
    "\n",
    "- Aktivierunsfunktion: ReLU\n",
    "- Filtergrößen: 3, 4, 5\n",
    "- Anzahl der Filter: 100\n",
    "- Dropoutrate: 0.5\n",
    "- Batchgröße: 50\n",
    "- Optimierungsverfahren: Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maschinelle Lernverfahren\n",
    "\n",
    "\n",
    "Für die Experimente in dieser Arbeit wurden neben KimCNN und einem *fine-tuned* BERT-Modell die Machine Learning Verfahren Logistic Regression und Support Vector Machines für die Sentiment Analysis verwendet. Sie sind etablierte Verfahren, die schon seit Jahrzehnten zur Textklassifikation verwendet werden (Kowsari, 2019, S. 3, 20f.). Anders als KimCNN oder das BERT-Modell nutzten diese Verfahren in dieser Arbeit keine vortrainierten Gewichte, sondern wurden \"from scratch\" trainiert, d.h. alle Informationen, die zur Klassifizierung verwendet wurden, wurden lediglich aus de Trainingsdaten gewonnen. Zur Vektorisierung der Wörter wurde anstelle von Word Embeddings ein Bag-of-Words Modell mit einer TF-IDF-Gewichtung verwendet, bei dem die Häufigkeit eines Wortes im Verhältnis zur Häufigkeit im gesamten Korpus berücksichtigt und gegebenenfalls niedriger gewichtet wird. Die Experimente mit den Machine Learning Verfahren sollten ein Gegenentwurf zu den anderen Ansätzen darstellen, da sie auf Informationen aus vortrainierten Embeddings verzichten und stattdessen nur die Informationen aus dem Nutzerreview-Korpus zur Sentiment Analysis verwenden. Weiterhin sollte überprüft werden, wie hoch die Auswirkung der simpleren Darstellung der Wörter durch hochdimensionale, dünnbesetzte Vektoren im Vergleich zu den weniger hochdimensionalen, dichtbesetzten Embbeddings hinsichtlich der Klassifizierungsgenauigkeit ausfallen würde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimente\n",
    "\n",
    "## Aufbau\n",
    "\n",
    "Für die Experimente wurden Stoppwörter beibehalten, um die Größe der im Durchschnitt sehr kurzen Reviews (~49 Tokens) nicht noch weiter zu verringern. Alle Wörter wurden durch kleingeschriebene Wörter dargestellt. Das Korpus wurde für die Experimente in einen Trainings-, Validierungs- und Testdatensatz aufgeteilt. Die Aufteilung erfolgte nach dem Pareto-Prinzip, d.h. 80% der Daten wurden als Trainingsdaten verwendet und jeweils 10% für die Validierungs- und Testdaten. Dies wurde dreimal durchgeführt, sodass eine dreifache Kreuzvalidierung durchgeführt werden konnte. Die Genauigkeit wurde bei jeder Kreuzvalidierung für den Testdatensatz berechnet und dann gemittelt. Für die Messung der Genauigkeit wurde die **Categorical Accuracy** verwendet, welche mit der folgenden Formel berechnet wird:<br>\n",
    "\n",
    "$\\text{Categorical Accuracy} = \\dfrac{\\text{Anzahl der korrekten Voraussagen}}{\\text{Gesamtanzahl aller Voraussagen}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die unterschiedlichen Embeddings wurden eine Reihe von vortrainierten Embeddings verwendet (siehe Tabelle 2). Für jedes Embedding (außer den BERT Embeddings) wurde in Kombination mit KimCNN eine Hyperparameteroptimierung durchgeführt, bei der folgende Parameter und Werte optimiert wurden:\n",
    "- Batchgröße: 50, 128\n",
    "- Lernrate: 0.01, 0.001\n",
    "- Maximale Anzahl an Features: 25000, 50000\n",
    "- Maximale Anzahl an Epochen: 500\n",
    "\n",
    "Die Batchgröße wurde einmal auf 50 gestellt, da dies bei den Experimenten von Kim die besten Ergebnisse lieferte (Kim, 2014, S. 1748) und als Vergleich auf eine größere Batchgröße von 128. Die maximale Anzahl an Epochen wurde auf 500 gestellt, jedoch wurde Early Stopping mit einer Patience von 3 eingebaut, sodass das Netz aufhörte zu trainieren, sobald sich der Validierungs Loss drei Epochen lang nicht verringerte. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "| Embedding | Modell                 | Details      | Link |\n",
    "|-----------|------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------|\n",
    "| **BERT**  | bert-base-uncased | 12-layer, 768-hidden, 12-heads, 110M parameters. Wurde auf kleingeschriebenen englischen Texten trainiert. | https://huggingface.co/transformers/pretrained_models.html (abgerufen am 23.08.2020). |\n",
    "| **FastText**  | fasttext.en.300d | Eine Million Wortvektoren wurden auf einem Wiki-Korpus von 2017, dem UMBC Korpus und dem statmt.org Nachrichtenkorpus erstellt. Die Sprache ist Englisch.  | https://fasttext.cc/docs/en/english-vectors.html (abgerufen am 23.08.2020).  |\n",
    "| **FastText**  | fasttext.simple.300d   | Eine Million Wortvektoren wurden auf einem Wiki-Korpus von 2017, dem UMBC Korpus und dem statmt.org Nachrichtenkorpus erstellt. Die Sprache ist einfaches Englisch.  |  https://fasttext.cc/docs/en/english-vectors.html (abgerufen am 23.08.2020). |\n",
    "| **GloVe**     | glove.6B.300d          | Diese Embeddings wurden auf einem Wiki-Korpus von 2014 und dem Gigaword Korpus trainiert. Es beinhaltet 6 Milliarden Tokens und ein Vokabular von 400000 Tokens.   |  https://nlp.stanford.edu/projects/glove/ (abgerufen am 23.08.2020) |\n",
    "| **GloVe**     | glove.840B.300d        | Diese Embeddings wurden auf dem Common Crawl Korpus trainiert. Es beinhaltet 840 Milliarden Tokens und ein Vokabular von 2,2 Millionen Tokens. | https://nlp.stanford.edu/projects/glove/ (abgerufen am 23.08.2020)                                |\n",
    "| **GloVe**     | glove.twitter.27B.200d | Diese Embeddings wurden auf Tweets von Twitter trainiert. Es beinhaltet 2 Milliarden Tweetw, 27 Milliarden Tokens und ein Vokabular von 1,2 Millionen Tokens.  | https://nlp.stanford.edu/projects/glove/ (abgerufen am 23.08.2020) | \n",
    "\n",
    "**Tabelle 2**: Auflistung aller verwendeten Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Experimente mit dem fine-tuned BERT-Modell wurde versucht, sich an die vorgeschlagenen Hyperparameter von Devlin et. al zu halten (Devlin, 2018, S. 4183f.), die im Folgenden aufgelistet werden:\n",
    "\n",
    "- Batchgröße: 16, 32\n",
    "- Lernrate: 2e-5, 3e-5, 5e-5\n",
    "- Anzahl der Epochen: 2, 3, 4\n",
    "- Optimierungsverfahren: AdamW\n",
    "- Maximale Anzahl an Features: 510\n",
    "\n",
    "Die Batchgröße wurde für die Experimente in dieser Arbeit auf $8$ reduziert. Die Anzahl der Epochen wurde auf $10$ erhöht und wie bei den Experimenten mit KimCNN wurde EarlyStopping mit einer Patience von $2$ in das Training integriert. Die maximale Anzahl an Features ist kleiner als bei den anderen Experimenten, da BERT standardmäßig nur eine maximale Textlänge von 512 Tokens(FN: Tatsächlich sind es nur 510 Tokens, da noch jeweils ein Token für unbekannte Wörter <UNK> und ein Padding-Token <PAD> hinzukommen.) erlaubt. Für BERT stehen mehrere vortrainierte Modelle zur Verfügung (FN: Siehe https://huggingface.co/transformers/pretrained_models.html (abgerufen am 27.08.2020).), in dieser Arbeit wurde nur das vortrainierte Modell \"bert-base-uncased\" verwendet.(FN: Es wurden auch Versuche mit \"bert-large-uncased\" gestartet, jedoch erzeugten diese sehr schlechte Ergebnisse, da BERT Large sehr viel Ressourcen benötigt und somit nur eine geringe Batchgröße (= $2$) ausgewählt werden konnte, siehe auch https://github.com/google-research/bert#out-of-memory-issues (abgerufen am 27.08.2020).)\n",
    "\n",
    "Für die Experimente mit den Machine Learning Verfahren wurde für die Evaluation und die Hyperparameteroptimierung eine verschachtelte Kreuzvalidierung (FN: Dabei wählt die \"innere\" Kreuzvalidierung auf Basis der zu untersuchenden Hyperparameter das beste Modell aus und die \"äußere\" Kreuzvalidierung misst die Performance des jeweils besten Modells.) durchgeführt, indem die zu optimierenden Hyperparameter, die Suchräume sowie eine Pipeline mit der TF-IDF-Vektorisierung und dem jeweiligen Klassifizierungsverfahren der Kreuzvalidierung übergeben wurde. Durch die verschachtelte Kreuzvalidierung sollte eine möglichst unverfälschte Bewertung der Machine Learning Verfahren gewährleistet werden. Es wurden folgene Parameter in den Suchräumen optimiert: \n",
    "\n",
    "- N-Gramme: (1,1), (1,2), (2,3)\n",
    "- Maximale Anzahl an Features: 25000, 50000\n",
    "- Toleranz: 0.01, 0.001\n",
    "- C: 1, 2, 3\n",
    "- Maximale Anzahl an Iterationen: 1000, 3000, 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergebnisse\n",
    "\n",
    "\n",
    "TODO: ausfüllen\n",
    "\n",
    "\n",
    "\n",
    "| Modell (+ Embedding) | Genauigkeit | Beste Parameter |\n",
    "|------------------------|------------|-------------------------------------------------------------------------------------------------------------------------|\n",
    "| BERT + bert-base-uncased | 76,4 | Batchgröße: 8<br>Lernrate: 2e-05<br>Maximale Anzahl an Features: 510|\n",
    "| KimCNN + fasttext.en.300d | 71,52 | Batchgröße: 128<br>Lernrate: 0.01<br>Maximale Anzahl an Features: 50000 |\n",
    "| KimCNN + fasttext.simple.300d | 71,29 | Batchgröße: 128<br>Lernrate: 0.01<br>Maximale Anzahl an Features: 25000 |\n",
    "| KimCNN + glove.6B.300d | 71,2 | Batchgröße: 128<br>Lernrate: 0.01<br>Maximale Anzahl an Features: 25000 |\n",
    "| KimCNN + glove.840B.300d | 71,24 | Batchgröße: 128<br>Lernrate: 0.01 <br>Maximale Anzahl an Features: 50000 |\n",
    "| KimCNN + glove.twitter.27B.200d | 70,86 | Batchgröße: 50<br>Lernrate: 0.01<br>Maximale Anzahl an Features: 50000 |\n",
    "| Logistic Regression | 72,4 | - |\n",
    "| Support Vector Machines | 71 | - |\n",
    "\n",
    "**Tabelle 3**: Die Tabelle stellt in jeder Zeile die durschnittlichen Genauigkeiten des jeweils besten Modells sowie die besten Parameter dar. Aufgrund der verschachtelten Kreuzvalidierung konnten die besten Hyperparameter für die maschinellen Lernverfahren Logistic Regression und Support Vector Machines nicht ermittelt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tabelle 3 werden alle verwendeten Verfahren und die Klassifizierungsgenauigkeiten des jeweiligen Verfahrens mit den besten Hyperparametern sowie die besten Hyperparameter dargestellt. \n",
    "\n",
    "\n",
    "\n",
    "TODO: \n",
    "- Tabelle\n",
    "- interpretieren\n",
    "    - bei fasttext: \n",
    "        - keine großen unterschiede bei max features und batch size\n",
    "        - insgesamt sehr stabile und gleichbleibende ergebnisse (+/- 2% acc)\n",
    "        - learning rate ist eig immer 0.01 besser\n",
    "    - bei glove:\n",
    "    - bei bert\n",
    "    - die ml verfahren sind genauso gut wie kimcnns mit embeddings    \n",
    "\n",
    "\n",
    "TODO: hier confusion matrices\n",
    "- bestes cnn noch ausführlicher optimieren\n",
    "- ausgewählte confusion matrix von bestem cnn und bestem bert hier zeigen\n",
    "    - Bei BERT sind die stärksten Fehlklassifizierungen bei Neutral und Negativ. TODO: cnn auch?\n",
    "- unterschiede angucken\n",
    "\n",
    "TODO: Besonderheiten beim Nutzerreview-Korpus sind die Kürze der Texte und die fehlerhafte Orthographie.\n",
    "\n",
    "Neben der reinen Klassifizierungsgenauigkeit sollten noch die Dauer des Trainingsm, die Dauer der Hyperparameteroptimierung und die benötigte Rechenleistung untersucht werden. Dabei sind die gemessenen Zeitwerte nur ungefähre Richtwerte, da eine genaue Messung der Trainings- und Optimierungsdauer viele Faktoren wie die Auslastung oder die eingebauten Hardwarekomponenten des Computers berücksichtigen muss. Die schnellsten Verfahren inklusive Hyperparameteroptimierung waren die Machine Learning Verfahren, die im Schnitt für das Training mit Hyperparameteroptimierung etwa 20 Minuten brauchten. Als einzige Verfahren konnten diese nur auf der CPU trainiert werden, für KimCNN und BERT war zusätzlich eine GPU nötig, um das Training zu beschleunigen. Trotzdem brauchte KimCNN für jedes Trainings eines Embeddingmodells inklusive Hyperparameteriptimierung etwa 15-20 Stunden. BERT benötigte etwa genau so lange wie KimCNN, jedoch wurden auch weitaus weniger Hyperparameter optimiert. Ist die Trainings- und Optimierungsdauer also ein wichtiger Faktor, so sind die Machine Learning Verfahren effizienter als KimCNN, da sie nicht nur schneller waren, sondern auch gleiche Klassifizierungsgenauigkeiten erreichen konnten und keine GPU benötigten. BERT benötigt ebenfalls eine GPU und erlaubt aufgrund seiner Komplexität nur geringe Batchgrößen, konnte jedoch höhere Klassifizierungsgenauigkeiten als die Machine Learning Verfahren oder die KimCNNs erreichen (TODO: stimmt das?), auch wenn diese nicht bedeutend höher ausfielen (+ ~4% TODO)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schlussbetrachtung\n",
    "\n",
    "In dieser Arbeit wurde eine Sentiment Analysis anhand eines Amazon-Nutzerreview Korpus in der Produktkategorie \"Elektronik\" durchgeführt, wobei die Sentiment Analysis als Textklassifikation aufgefasst wurde. Für die Sentiment Analysis wurden verschiedene Klassifizierungstechniken verwendet und miteinander verglichen. Jede dieser Klassifizierungstechniken unterschied sich von den anderen Techniken. Die Machine Learning Verfahren verwendeten anders als die anderen Verfahren keine vortrainierten Wordvektoren, sondern erhielten alle zur Klassifizierung benötigten Informationen lediglich aus den Trainingsdaten. KimCNN ist eine simple Variante eines Convolutional Neural Networks für Textdaten und konnte 2014 für einige Textklassifikationsaufgaben State-of-the-art Ergebnisse erzielen. Anders als bei den Machine Learning Verfahren wurden hier eine Reihe verschiedener vortrainierter Embeddingmodelle der Embeddingverfahren GloVe und FastText für die Sentiment Analysis verwendet. Die Idee war, dass die Word Embeddings durch ihre Berücksichtigung des Wortkontextes zu einer Verbesserung der Klassifizierungsgenauigkeit beitrugen, da so Schlüsselwörter oder -phrasen besser erkannt und zugeordnet werden konnten. Die letzte verwendete Klassifizierungstechnik war ein *fine-tuned* BERT-Modell. BERT ist seit zwei Jahren sehr populär, da es bei seiner Veröffentlichung 2018 eine Reihe von State-of-the-art Ergebnisse im Bereich des Natural Language Processing erreichen konnte. Auch BERT benutzt Word Embeddings, jedoch sind diese Embeddings in der Lage, dynamisch den Kontext eines Wortes zu repräsentieren und somit Worte mit gleicher Schreibweise aber unterschiedlicher Bedeutung voneinander zu unterscheiden.\n",
    "\n",
    "TODO: die Ergebnisse nennen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literaturverzeichnis\n",
    "\n",
    "BOJANOWSKI, Piotr, GRAVE, Edouard, JOULIN, Armand, MIKOLOV, Tomas, \"Enriching Word Vectors with Subword Information\", in: Transactions of the Association for Computational Linguistics, Bd. 5, Juli 2016, S. 135-146.\n",
    "\n",
    "CHOLLET, Francois, Deep learning with Python, 2018.\n",
    "\n",
    "DEVLIN, Jacob, CHANG, Ming-Wei, LEE, Kenton, TOUTANOVA, Kristin, \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", in: Proceedings of the NAACL-HLT Conference, S. 4171–4186.\n",
    "\n",
    "GéRON, Aurélien, Praxiseinstieg Machine Learning mit Scikit-Learn, Keras und TensorFlow, übers. v. Kristian Rother und Thomas Demmig, ${^2}2020$.\n",
    "\n",
    "GOLDBERG, Yoav, A Primer on Neural Network Models for Natural Language Processing, in: Journal Of Artificial Intelligence Research, Bd. 57 (2016), S. 345-420.\n",
    "\n",
    "KARN, Ujjwal, An Intuitive Explanation of Convolutional Neural Networks, https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/ (abgerufen am 20.08.2020).\n",
    "\n",
    "KIM, Yoon, Convolutional Neural Networks for Sentence Classification, in: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (Oktober 2014), S. 1746-1751.\n",
    "\n",
    "KOWSARI, Kamran et . al., Text Classification Algorithms. A Survey, ArXiv 2019, S. 1-68.\n",
    "\n",
    "JACOVI, Alon, SHALOM, Oren Sar, GOLDBERG, Yoav, Understanding Convolutional Neural Networks for Text Classification, in: Proceedings of the 2018 EMNLP Workshop BlackboxNLP. Analyzing and Interpreting Neural Networks for NLP (Januar 2018), S. 56-65.\n",
    "\n",
    "\n",
    "LIU, Bing, Sentiment analysis. Mining opinions, sentiments, and emotions, Cambridge 2015.\n",
    "\n",
    "PENNIGTON, Jeffrey, SOCHER, Richard, MANNING, Christopher D., \"GloVe: Global Vectors for Word Representation\", in: EMNLP (Januar 2014), S. 1532-1533.\n",
    "\n",
    "PETROLITO, Ruggero, DELL'ORLETTA, Felice, Word Embeddings in Sentiment Analysis, in: Proceedings of the Fifth Italian Conference on Computational Linguistics CLiC-it (Januar 2018), S. 330-334.\n",
    "\n",
    "PILEHVAR, Mohammad Taher, CAMACHO-COLLADOS, Jose, \"Embeddings in Natural Language Processing. Theory and Advances in Vector Representation of Meaning\", 2020.\n",
    "\n",
    "WEIDMAN, Seth, Deep Learning. Grundlagen und Implementierung, übers. v. Jorgen W. Lang, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Daten und Code\n",
    "\n",
    "Alle Daten und alle zur Klassifizierung verwendeten Skripte befinden sich im Github-Repository `wordembeddings`.(FN: Siehe https://github.com/realjanpaulus/wordembeddings.) In der folgenden Auflistung werden die Ordner und ihre Inhalte kurz erläutert:\n",
    "\n",
    "- **app**. In diesem Ordner befinden sich die zur Klassifizierung und Optimierung verwendeten Skripte `cnn.py`, `bert.py`, `ml.py`, `run.py`, `run_bert.py` sowie das Helferskript `utils.py` und die Datei `models.py`, welche die KimCNN Architektur beinhaltet. Das Notebook `processing_and_results.ipynb` wurde für die Vorverarbeitung des Korpus und zur Zusammenfassung der Klassifizierungsergebnisse genutzt.\n",
    "- **corpora**. In diesem Ordner befindet sich das Amazon-Nutzerreview-Korpus und die für die Kreuzvalidierung verwendeten Teilkorpora.\n",
    "- **results**. In diesem Ordner befinden sich die Ergebnisse der Klassifizierungsexperimente als Textdateien sowie Grafiken der Konfusionsmatrizen und des Trainings- und Validierungs-Loss der CNNs und des BERT-Modells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Inhaltsverzeichnis",
   "title_sidebar": "Inhalte",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "213.8249969482422px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
